{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TcJnn2vVLF"
      },
      "source": [
        "# COMP2211 PA1: K-Means Clustering and K-Nearest Neighbors for Forest Cover Classification\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The Roosevelt National Forest of northern Colorado is characterized by clusters of varying tree species (also known as Forest Covers). It has been found that cartographic data (or the information used to draw maps) can be reliable indicators for the kinds of tree species found in specific parts of the forest. In this PA, we will use cartographic data to categorize the Forest Cover Types over parts of the Roosevelt National Forest.\n",
        "\n",
        "## Task Overview\n",
        "\n",
        "We will begin by classifying forest cover types using **K-Nearest Neighbors**. Then, we will further analyze the data through **K-Means Clustering**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syQBE4RfKEzJ"
      },
      "source": [
        "## Task 0: Setting up\n",
        "\n",
        "First, we need to upload all the relevant libraries.\n",
        "\n",
        "Note: This part will not be graded!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgiczKIHKxxe"
      },
      "source": [
        "### Task 0.1: Import libraries\n",
        "It's a good habit to import all libraries at the beginning of the code, and it helps in the following aspects:\n",
        "*   Readability and clarity\n",
        "*   Avoiding namespace clashes\n",
        "*   Dependency management\n",
        "*   Consistency and convention\n",
        "\n",
        "**Todo:**  \n",
        "Please import your libraries in the following cell.  \n",
        "\n",
        "**Remarks:**\n",
        "1. We use [Numpy](https://numpy.org/) and [Pandas](https://pandas.pydata.org/) in this PA. You may also import other modules as long as they are part of the [Python Standard Library](https://docs.python.org/3/library/).  \n",
        "2. You are NOT allowed to use any other external libraries/functions\n",
        " (especially any machine learning library, e.g., sklearn) in todo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U-eZ5cl6LfwY"
      },
      "outputs": [],
      "source": [
        "# task 0.1: import libraries\n",
        "# todo start #\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# todo end #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZcaiiqAMAOQ"
      },
      "source": [
        "### Task 0.2: Read Dataset\n",
        "Now you have the needed libraries in hand. Next, read the dataset from the source file to the project.  \n",
        "\n",
        "We assume you are working in Google Colab. One way to read a dataset in Google Colab:\n",
        "1. Download the source file and put it on your Google Drive\n",
        "2. Import the `drive` module from `google.colab` package\n",
        "3. Run `drive.mount` to mount your Google Drive to the Colab notebook\n",
        "4. Use `pandas.read_csv` to read the data from Google Drive and store the data in pandas DataFrame\n",
        "\n",
        "**Todo:** \\\n",
        "Modify `YourFilePath` depending on the directory to read the data to this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOTpICU-MSZK",
        "outputId": "da1cd504-0a48-4260-dfcf-3139b80f4c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Task 0.2: read dataset\n",
        "if __name__ == '__main__':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "# todo start #\n",
        "    YourFilePath = # Fill in the missing code/s #\n",
        "# todo end #\n",
        "    train_features = pd.read_csv('/content/drive/MyDrive/'+YourFilePath+'/train_features.csv')\n",
        "    test_features = pd.read_csv('/content/drive/MyDrive/'+YourFilePath+'/test_features.csv')\n",
        "    train_labels = pd.read_csv('/content/drive/MyDrive/'+YourFilePath+'/train_labels.csv')\n",
        "    test_labels = pd.read_csv('/content/drive/MyDrive/'+YourFilePath+'/test_labels.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwg46jpovYRn"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "This dataset is for the task of Forest Cover Type Classification. The 7 Forest Cover Types are represented by integers (1-7):\n",
        "\n",
        "*   1 - Spruce/Fir\n",
        "*   2 - Lodgepole Pine\n",
        "*   3 - Ponderosa Pine\n",
        "*   4 - Cottonwood/Willow\n",
        "*   5 - Aspen\n",
        "*   6 - Douglas-fir\n",
        "*   7 - Krummholz\n",
        "\n",
        "These integers are the labels to be predicted. The labels for the training set can be found in the data file *train_labels.csv*, and the labels for the test set can be found in the data file *test_labels.csv*\n",
        "\n",
        "The goal is to predict the Forest Cover type using cartographic or mapping data. These can be found in the files *train_features.csv* and *test_features.csv*\n",
        "\n",
        "You can check the features using the Pandas's *dataframe.describe()*. This will show you that the features have a very different range of values (compare Elevation with Slope). Because of this, we need to perform some data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYMBXt6-jOgs",
        "outputId": "b7514af2-8e70-4091-dc31-596d367feaf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Elevation        Aspect         Slope  \\\n",
            "count  12096.000000  12096.000000  12096.000000   \n",
            "mean    2748.303406    156.311177     16.510499   \n",
            "std      417.822995    109.903370      8.471222   \n",
            "min     1863.000000      0.000000      0.000000   \n",
            "25%     2374.000000     65.000000     10.000000   \n",
            "50%     2751.000000    125.000000     15.000000   \n",
            "75%     3102.000000    259.000000     22.000000   \n",
            "max     3849.000000    360.000000     50.000000   \n",
            "\n",
            "       Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  \\\n",
            "count                      12096.000000                    12096.000000   \n",
            "mean                         227.905754                       51.165923   \n",
            "std                          210.565445                       61.547238   \n",
            "min                            0.000000                     -146.000000   \n",
            "25%                           67.000000                        5.000000   \n",
            "50%                          180.000000                       32.000000   \n",
            "75%                          330.000000                       79.000000   \n",
            "max                         1343.000000                      554.000000   \n",
            "\n",
            "       Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  \\\n",
            "count                     12096.000000   12096.000000    12096.000000   \n",
            "mean                       1714.041501     212.864501      218.889550   \n",
            "std                        1329.915391      30.498605       22.787213   \n",
            "min                           0.000000      58.000000       99.000000   \n",
            "25%                         760.000000     196.000000      207.000000   \n",
            "50%                        1308.000000     220.000000      222.000000   \n",
            "75%                        2271.000000     235.000000      235.000000   \n",
            "max                        6836.000000     254.000000      254.000000   \n",
            "\n",
            "       Hillshade_3pm  Horizontal_Distance_To_Fire_Points     Soil_Type  \n",
            "count   12096.000000                        12096.000000  12096.000000  \n",
            "mean      134.802249                         1509.822503     19.148065  \n",
            "std        45.980947                         1096.319966     12.628170  \n",
            "min         0.000000                            0.000000      1.000000  \n",
            "25%       106.000000                          732.000000     10.000000  \n",
            "50%       138.000000                         1256.000000     17.000000  \n",
            "75%       167.000000                         1989.000000     30.000000  \n",
            "max       248.000000                         6993.000000     40.000000  \n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  print(train_features.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCKQ_gR82tex"
      },
      "source": [
        "## Task 1: Data Preprocessing\n",
        "\n",
        "Data preprocessing ensures the fair treatment of features, efficient computation, and easier interpretability among features. For this assignment, we will use **Z-score Normalization**.\n",
        "\n",
        "**Note:** For this assignment, you can treat all the features as numerical features.\n",
        "\n",
        "Suppose $X:(x_1, x_2, ..., x_n)$ is a column (corresponding to a feature), then\n",
        "$\\displaystyle X_{\\text{Z-score-normalized}} = \\frac{X-\\mu_X}{\\sigma_X}$\n",
        "\n",
        "**Todo:**  \n",
        "Implement `z_score_normalization(input_array)`.  \n",
        "\n",
        "**Suggested Numpy functions:**\n",
        "`numpy.mean`, `numpy.std` ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kOK7u5Ysn2AN"
      },
      "outputs": [],
      "source": [
        "# Task 1: Data Preprocessing\n",
        "def z_score_normalization(input_array):\n",
        "  # input_array: numpy array of shape (num_rows, num_features)\n",
        "  # todo start #\n",
        "\n",
        "  # todo end #\n",
        "  return normalized_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqU1bPWstqvo"
      },
      "source": [
        "## Task 2: Measuring \"Nearness\" of Neighbors\n",
        "\n",
        "Now, we can check if it's possible to determine forest cover based on cartographical area. The forest cover serves as a label for the data point. It is possible that cartographical information may be good features for classifying forest cover types.\n",
        "\n",
        "In **K-Nearest Neighbors**, a test sample is classified based on the \"distance\" of its features to the features of labelled training samples. The predicted class is based on the label of the majority of the K nearest neighbors.\n",
        "\n",
        "Evidently, this involves calculating distance measures. Aside from Euclidean distance, here are two other distance metrics that can be used in K-Nearest Neighbors:\n",
        "\n",
        "### Task 2.1: Manhattan Distance\n",
        "$\\displaystyle d(X, Y) = \\sum_{i=1}^{n} | x_i - y_i |$\n",
        "\n",
        "### Task 2.2: Cosine Distance\n",
        "$\\displaystyle d(X, Y) = 1 - \\frac{ \\sum_{i=1}^{n} x_i \\times y_i }{ \\sqrt{\\sum_{i=1}^{n} x_i^2 } \\sqrt{\\sum_{i=1}^{n} y_i^2 }}$\n",
        "\n",
        "**Todo:** \\\n",
        "Implement the functions `manhattan_distance` and `cosine_distance`, which will calculate the distance of each training sample in `X_train` to each test sample in `X_test` based on their feature values.\n",
        "\n",
        "**Suggested Numpy functions:** \\\n",
        "`numpy.expand_dims`, `numpy.dot`, `numpy.sqrt`, `numpy.sum` ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jy1ODj4OiqlX"
      },
      "outputs": [],
      "source": [
        " # Task 2.1: Manhattan distance\n",
        "def manhattan_distance(X_train, X_test):\n",
        "  # X_train: numpy array of shape (num_rows_train, num_features)\n",
        "  # X_test: numpy array of shape (num_rows_test, num_features)\n",
        "  # todo start #\n",
        "  \n",
        "  # todo end #\n",
        "  return distance\n",
        "  # distance: numpy array of shape (num_rows_test, num_rows_train)\n",
        "\n",
        "# Task 2.2: Cosine distance\n",
        "def cosine_distance(X_train, X_test):\n",
        "  # X_train: numpy array of shape (num_rows_train, num_features)\n",
        "  # X_test: numpy array of shape (num_rows_test, num_features)\n",
        "  # todo start #\n",
        "  \n",
        "  # todo end #\n",
        "  return distance\n",
        "  # distance: numpy array of shape (num_rows_test, num_rows_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syL7ldZJjgKU"
      },
      "source": [
        "## Task 3: Classification based on the Nearest Neighbors\n",
        "\n",
        "After calculating the distance measures, it is now possible to get the index of the K Nearest Neighbors. More importantly, we can identify the labels of the K Nearest Neighbors. You can do this very efficiently using a few Numpy functions.\n",
        "\n",
        "**Todo:**\n",
        "* Implement the function `knn_prediction` that predicts the class of the test samples `X_test`. The predicted class will be based on the majority class of its nearest neighbors.\n",
        "* If there are ties in the number of classes, calculate an inverse distance weight to break the tie:\n",
        "  * For example, let `k = 5` and let the nearest neighbors for a test sample be `neighbors = [1, 1, 2, 2, 3]`\n",
        "  * Because the prediction is tied between Class 1 and Class 2, we calculate the distance of the test sample to its nearest neighbors. Let the distance measures corresponding to each value in `neighbors` be `distances = [5, 6, 7, 8, 10]`\n",
        "  * The inverse distances will be `inv_distances = [0.2, 0.16666667, 0.14285714, 0.125, 0.1]`\n",
        "  * Calculate the **inverse distance weight** as the sum of the inverse distances for each class. So, the inverse distance weight for Class 1 would be 0.36666667, and for Class 2 it would be 0.26785714.\n",
        "  * Since Class 1 has the larger inverse distance weight, the predicted class is Class 1.\n",
        "* You can assume there will be no ties in distance.\n",
        "* The function should also return `y_classes`, which is just an array of all the possible values of `y_train`.\n",
        "\n",
        "**Suggested Numpy functions:** `numpy.arange`, `numpy.argsort`, `numpy.take`, `numpy.expand_dims`, `numpy.where`...\n",
        "\n",
        "**Suggested approach:**\n",
        "1. Determine the `k` nearest neighbors for each sample in `y_test`\n",
        "2. Count the number of neighbors belonging to each class and identify the majority class (i.e., the one with the most number of neighbors)\n",
        "3. Check if the number of neighbors in the other classes is equal to the number of neighbors in the majority class, resulting in a tie\n",
        "4. If there is a tie, calculate the **inverse distance weight** for those samples and break the tie\n",
        "\n",
        "**Programming challenge for this part:** Accomplish this task without using for loops. This will teach you more efficient ways to write code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FpWwg3VaLYi9"
      },
      "outputs": [],
      "source": [
        "# Task 3: Classification based on the Nearest Neighbors\n",
        "def knn_prediction(distances, y_train, k):\n",
        "  # distance: numpy array of shape (num_rows_test, num_rows_train), return value from previous distance functions\n",
        "  # y_train: numpy array of shape (num_rows_train, ),  the labels of training data\n",
        "  # k: integer, k in \"K-nearest neighbors\"\n",
        "\n",
        "  # todo start #\n",
        "  \n",
        "  # todo end #\n",
        "\n",
        "  return prediction, y_classes\n",
        "  # prediction: 1-D numpy array of shape (num_rows_test, )\n",
        "  # y_classes: 1-D numpy array of shape (num_classes, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrAh87Qf-MI3"
      },
      "source": [
        "## Task 4: Evaluation of the K Nearest Neighbors Classifier\n",
        "\n",
        "Now it is time to evaluate the classifier you made in the previous task. In this task, you will make this evaluation by calculating the [F-score](https://en.wikipedia.org/wiki/F-score).\n",
        "\n",
        "In summary, here are the relevant calculations for the F-score.\n",
        "$$\\displaystyle precision = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Positive}} $$\n",
        "\n",
        "$$\\displaystyle recall = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} $$\n",
        "\n",
        "$$\\displaystyle F\\text{-score} = \\frac{2 * precision * recall}{precision+recall}$$\n",
        "\n",
        "**Note:**\n",
        "* This means each class has its own F-score.\n",
        "* If some classes do not appear in the `X_test` (i.e., $\\text{True Positive} + \\text{False Positive}$ **or** $\\text{True Positive} + \\text{False Negative}$ are 0), you will get a value `np.nan`.\n",
        "  * Add the code `np.finfo(float).eps` to the denominator when calculating $precision$ or $recall$. This is a very small value approaching 0.\n",
        "  * For example, the calculation for $precision$ would become:\n",
        "  $$\\displaystyle precision = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Positive } + \\epsilon }  $$\n",
        "  \n",
        "\n",
        "\n",
        "**Todo:** \\\n",
        "Implement a function `f_score` to calculate the accuracy of the classifier. A score of 1.0 indicates perfect precision and recall, while a score of 0.0 indicates 0 precision or recall.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3IBRVc9yGXoe"
      },
      "outputs": [],
      "source": [
        "# Task 4: Evaluation of the K Nearest Neighbors Classifier\n",
        "def f_score(y_test, prediction, y_classes):\n",
        "  num_classes = len(y_classes)\n",
        "\n",
        "  # todo start #\n",
        "  f_score_array = np.zeros(num_classes)\n",
        "  for i in range(num_classes):\n",
        "      # Fill in the missing code/s #\n",
        "\n",
        "      f_score_array[i] = f1_score\n",
        "    # todo end #\n",
        "\n",
        "  return f_score_array\n",
        "  # f_score: 1-D array with shape (num_classes, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E_5kJYWmCgv"
      },
      "source": [
        "## Task 5: Assigning Data Points to Clusters\n",
        "\n",
        "In **K-Means Clustering**, data points are assigned to clusters based on their distances to the cluster centroid. We will divide this process into the following steps:\n",
        "\n",
        "### Task 5.1: Euclidean Distance Calculation\n",
        "\n",
        "Typically, Euclidean distance is used in K-Means Clustering as the model may fail to converge in some cases when other distance measures are used.\n",
        "\n",
        "The Euclidean distance equation is given below. Suppose we are calculating the distance between a data point $X:(x_1, x_2, ... ,x_n)$ and a cluster centroid $C:(c_1, c_2, ... c_n)$ in n-dimensional space.\n",
        "\n",
        "$$\\displaystyle d(X, C) = \\sqrt{\\sum_{i=1}^{n} (x_i - c_i)^2}$$\n",
        "\n",
        "This calculation is performed for each sample in the training set.\n",
        "\n",
        "### Task 5.2: Cluster Assignment\n",
        "\n",
        "Once the cluster distances have been calculated, we can assign the data points to clusters. Each data point is simply **assigned to the cluster to which it has the minimum distance**.\n",
        "\n",
        "### Task 5.3: Calculate New Centroids\n",
        "\n",
        "After the data points have been assigned to their new clusters, these new cluster assignments will be used to determine the new centroids. The new centroid is simply the **mean of the data point features assigned to that cluster**.\n",
        "\n",
        "**Programming challenge for this part:** Accomplish this task without using for loops. This will teach you more efficient ways to write code.\n",
        "\n",
        "**Todo:**\n",
        "* Implement the function `centroid_euclidean_distance` that calculates the distance between each data point and the centroid.\n",
        "* Implement the function `cluster_assignment` that returns the index of the cluster assignment for each data point.\n",
        "* Implement the function `calculate_centroids` that returns the new centroid for each cluster.\n",
        "* Make your solution as efficient as possible (i.e., minimize redundant code, reduce for loops.)\n",
        "\n",
        "**Suggested Numpy functions:**\n",
        "`numpy.square`, `numpy.sum`, `numpy.sqrt`, `numpy.argmin`, `numpy.newaxis`, `numpy.arange` ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AUfhmAJ9ntZV"
      },
      "outputs": [],
      "source": [
        "# Task 5.1: Euclidean Distance Calculation\n",
        "def centroid_euclidean_distance(X_train, centroids):\n",
        "  # X_train: numpy array of shape (num_rows_train, num_features)\n",
        "  # centroids: numpy array of shape (num_clusters, num_features)\n",
        "  # todo start #\n",
        "  \n",
        "  # todo end #\n",
        "  return distance\n",
        "  # distance: numpy array of shape (num_clusters, num_rows_train)\n",
        "\n",
        "# Task 5.2: Cluster Assignment\n",
        "def cluster_assignment(distance):\n",
        "  # distance: numpy array of shape (num_clusters, num_rows_train)\n",
        "  # todo start #\n",
        "  \n",
        "  # todo end #\n",
        "  return assignments\n",
        "  # assignment: 1-D numpy array of shape (num_rows_train, )\n",
        "\n",
        "# Task 5.3: Calculate New Centroids\n",
        "def calculate_centroids(X_train, assignment, k):\n",
        "  # X_train: numpy array of shape (num_rows_train, num_features)\n",
        "  # assignment: 1-D numpy array of shape (num_rows_train, )\n",
        "  # k: a scalar value for the number of clusters (NOTE: Include empty clusters in counting k)\n",
        "  # todo start #\n",
        "  \n",
        "  # todo end #\n",
        "  return new_centroids\n",
        "  # new_centroids: numpy array of shape (num_clusters, num_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajfKVFDJnh_1"
      },
      "source": [
        "## Task 6: Improving the K-Means Clustering Model\n",
        "\n",
        "Usually, we need to run the K-Means Clustering algorithm a few times to find better centroids. This means we will repeatedly apply the functions in Task 5 until a stopping criterion is met.\n",
        "\n",
        "We will now try to refine the K-Means Clustering model until some common stopping criteria are met:\n",
        "1. **Task 6.1: Stop When There Are No More Cluster Reassignments**\n",
        "2. **Task 6.2: Stop When Centroid Change is Below Threshold**\n",
        "\n",
        "**Todo:**\n",
        "* Implement a function `k_means_cluster_reassignment` that continuously refines the centroid until the current and previous iterations result in the same cluster assignments for each data point.\n",
        "* Implement a function `k_means_centroid_value` that continuously refines the centroid until the current and previous iterations result in roughly the same centroid values for each feature (with a maximum allowable difference of `threshold_value`).\n",
        "* The functions should return:\n",
        "  * `assignment` - the final cluster assignments\n",
        "  * `centroid` - the final centroids\n",
        "* We also need to limit the number of iterations to `max_iterations` in case the model fails to converge.\n",
        "* Reminder: You need to use the functions in Task 2 to implement these tasks.\n",
        "\n",
        "**Suggested methods:** `break`, `numpy.abs` ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SjpvxsAr19Cy"
      },
      "outputs": [],
      "source": [
        "# Task 6.1: Stop When There Are No More Cluster Reassignments\n",
        "def k_means_cluster_reassignment(X_train, initial_centroids, max_iterations=100):\n",
        "  # X_train: numpy array of shape (num_rows_train, num_features)\n",
        "  # initial_centroids: numpy array of shape (num_clusters, num_features)\n",
        "  # max_terations: the maximum number of iterations for refining the model\n",
        "  centroids = initial_centroids\n",
        "  k = centroids.shape[0]\n",
        "  for iteration in range(max_iterations):\n",
        "    distance = centroid_euclidean_distance(X_train, centroids)\n",
        "    assignment = cluster_assignment(distance)\n",
        "    centroids = calculate_centroids(X_train, assignment, k)\n",
        "    # todo start #\n",
        "\n",
        "    # todo end #\n",
        "  return assignment, centroids\n",
        "  # assignment: 1-D numpy array of shape (num_rows_train, )\n",
        "  # centroids: numpy array of shape (num_clusters, num_features)\n",
        "\n",
        "# Task 6.2: Stop When Centroid Change is Below Threshold\n",
        "def k_means_centroid_value(X_train, initial_centroids, max_iterations=100, threshold_value=0.0001):\n",
        "  # X_train: numpy array of shape (num_rows_train, num_features)\n",
        "  # initial_centroids: numpy array of shape (num_clusters, num_features)\n",
        "  # max_terations: the maximum number of iterations for refining the model\n",
        "  # threshold_value: a scaler value for the allowable difference between iterations\n",
        "  centroids = initial_centroids\n",
        "  k = centroids.shape[0]\n",
        "  for iteration in range(max_iterations):\n",
        "    distance = centroid_euclidean_distance(X_train, centroids)\n",
        "    assignment = cluster_assignment(distance)\n",
        "    centroids = calculate_centroids(X_train, assignment, k)\n",
        "    # todo start #\n",
        "    \n",
        "    # todo end #\n",
        "  return assignment, centroids\n",
        "  # assignment: 1-D numpy array of shape (num_rows_train, )\n",
        "  # centroids: numpy array of shape (num_clusters, num_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiL05Xu4kq4R"
      },
      "source": [
        "### Task 6.3: Evaluating the value of k\n",
        "\n",
        "Currently, we are arbitrarily deciding on the number of clusters (k). There are also metrics for identifying the best k. One of these is the silhouette score [(see link for more info)](https://en.wikipedia.org/wiki/Silhouette_(clustering)#:~:text=The%20silhouette%20score%20is%20specialized,distance%20or%20the%20Manhattan%20distance.). Below is a summary of the relevant formulas for calculating the silhouette score for a single value of k.\n",
        "\n",
        "For each data point $i$ in cluster $C_I$, define $a(i) = \\frac{1}{|C_I|-1} \\sum_{j \\in C_I, i \\neq j} d(i, j)$\n",
        "where $|C_I|$ is the number of data points in the cluster\n",
        "\n",
        "Then we define $b(i) = min_{J \\neq I} \\frac{1}{|C_J|} \\sum_{j \\in C_J} d(i,j) $\n",
        "\n",
        "So, the silhouette score of one data point is\n",
        "$$s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$$\n",
        "\n",
        "Then, the silhouette score for a specific value of k is calculated by taking the mean $s(i)$ over the entire dataset.\n",
        "\n",
        "**Todo:** \\\n",
        "Implement a function `silhouette_score_for_k` that calculates the silhouette score for a specific k and the k_means functions you made for previous tasks.\n",
        "\n",
        "**Warning:**\n",
        "If you are using Google Colab, Colab's CPU capacity may not be able to handle this computation. We recommend using a subset of `X_train`, e.g. `X_train[:300,:]` and `assignments[:300]`\n",
        "\n",
        "**Suggested methods:** `numpy.mean`, `numpy.min`, `numpy.maximum`, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yYLtPoWqku1f"
      },
      "outputs": [],
      "source": [
        "# Task 6.3: Evaluating the value of k\n",
        "def silhouette_score_for_k(X_train, assignments):\n",
        "  # Note: If you are using Google Colab, the CPU capacity may not be enough for large datasets\n",
        "  # X_train: numpy array of shape (num_rows_train, num_features)\n",
        "  # todo start #\n",
        "  n_samples = len(X_train)\n",
        "  silhouette_scores = []\n",
        "\n",
        "  for i in range(n_samples):\n",
        "      point = X_train[i]\n",
        "      assignment = assignments[i]\n",
        "      same_cluster_distances = np.sqrt(np.sum((X_train[assignments == assignment] - point) ** 2, axis=1))\n",
        "      same_cluster_distances = same_cluster_distances[same_cluster_distances != 0]\n",
        "\n",
        "\n",
        "      other_cluster_distances = []\n",
        "      for j in range(len(X_train)):\n",
        "          if assignments[j] != assignment:\n",
        "              other_cluster_distance = np.mean(np.sqrt(np.sum((X_train[assignments == assignments[j]] - point) ** 2, axis=1)))\n",
        "              other_cluster_distances.append(other_cluster_distance)\n",
        "\n",
        "      if len(other_cluster_distances) == 0:\n",
        "          silhouette_score = 0\n",
        "      else:\n",
        "          min_other_cluster_distance = np.min(other_cluster_distances)\n",
        "          silhouette_score = (min_other_cluster_distance - np.mean(same_cluster_distances)) / max(min_other_cluster_distance, np.mean(same_cluster_distances))\n",
        "\n",
        "      silhouette_scores.append(silhouette_score)\n",
        "      silhouette_coef = np.mean(silhouette_scores)\n",
        "  # todo end #\n",
        "  return silhouette_coef\n",
        "  # silhouette_coef: a scalar value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbsLtMZ1Nvo"
      },
      "source": [
        "## Final Reminder: ##\n",
        "* Review the changelog and FAQ on the <a href=\"https://course.cse.ust.hk/comp2211/assignments/pa1\">assignment webpage</a>.\n",
        "* While we provided you with some sample test cases on ZINC, the test cases used for final grading may be different. This means that if you hard code the answers, or make your model specific for this dataset in some way, your final PA1 grade may be much lower than the grade given by ZINC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPkIDs_7QrG0"
      },
      "source": [
        "## Playground: Try out your model here\n",
        "\n",
        "You can run the following codes to test your functions. This part will not be graded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLIZlI0QQ-Wz",
        "outputId": "33ae26c8-df1c-44b4-a76c-fa331bd06f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The normalized features of the first 5 samples of X_train are:  [[-3.64531641e-01 -9.58255697e-01 -1.59493603e+00  1.42927013e-01\n",
            "  -8.31362047e-01 -9.05389383e-01  2.66760906e-01  5.75366176e-01\n",
            "   2.87038390e-01  4.35034917e+00  7.80187617e-01]\n",
            " [-3.78892383e-01 -9.12759306e-01 -1.71298762e+00 -7.55414141e-02\n",
            "  -9.28852173e-01 -9.95624417e-01  2.33971167e-01  7.07024400e-01\n",
            "   3.52285503e-01  4.30109144e+00  7.80187617e-01]\n",
            " [ 1.33307398e-01 -1.57519217e-01 -8.86626443e-01  1.90420149e-01\n",
            "   2.24780993e-01  1.10234013e+00  6.93027524e-01  8.38682623e-01\n",
            "   4.30090096e-03  4.20622470e+00 -5.66064681e-01]\n",
            " [-3.66925098e-01 -1.01285137e+00 -1.71298762e+00 -3.55750918e-01\n",
            "  -8.47610401e-01 -9.94872458e-01  2.33971167e-01  6.63138325e-01\n",
            "   3.30536465e-01  4.25274589e+00  7.80187617e-01]\n",
            " [-4.05220409e-01 -2.21214165e-01 -1.24078123e+00  3.42398185e-01\n",
            "  -1.07508736e+00 -1.23850705e+00  5.61868565e-01  7.94796549e-01\n",
            "   1.13046089e-01  4.12412848e+00  7.80187617e-01]]\n",
            "The manhattan distance between the first 5 X_train and first 5 X_test are [[ 9.22482567  9.43327726  7.58189979  9.04521814  9.39654285]\n",
            " [13.57471088 13.40605565 14.85158874 13.08693309 13.23721594]\n",
            " [12.790188   12.60514013 14.34426235 12.71112859 12.75272006]\n",
            " [15.46424038 16.10906578 12.68923149 16.30536023 14.67838281]\n",
            " [11.90509162 12.5786385   8.93992083 12.75099839 11.88652964]]\n",
            "The cosine distance between the first 5 X_train and first 5 X_test are [[0.36141321 0.36115748 0.21135551 0.34743379 0.43599088]\n",
            " [1.00393406 0.97004442 1.08769914 0.97148336 0.91019346]\n",
            " [1.01493531 0.98526882 1.15373734 0.9947695  0.96234311]\n",
            " [1.15816915 1.18215995 1.00670396 1.19932262 1.15119441]\n",
            " [0.60705742 0.64003271 0.3159043  0.64873329 0.69229729]]\n",
            "The predicted classes for the first 10 samples in X_test are  [2 6 7 4 2 1 4 1 6 5]\n",
            "The true classes of these samples are  [2 6 7 4 2 2 4 1 6 5]\n",
            "The F-scores for each class are  [0.595      0.54521277 0.6907818  0.86985539 0.82890542 0.70232558\n",
            " 0.89030884]\n",
            "With cluster reassignment as the stopping criteria, the assignments for the first 5 samples of X_train are:  [2 2 2 2 2]\n",
            "With centroid value as the stopping criteria, the assignments for the first 5 samples of X_train are:  [2 2 2 2 2]\n",
            "The silhouette coefficient is  0.10381751140231042\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  X_train = np.array(train_features)\n",
        "  X_test = np.array(test_features)\n",
        "  y_train = np.array(train_labels)\n",
        "  y_test = np.array(test_labels)\n",
        "  np.random.seed(0)\n",
        "  n_clusters=3\n",
        "  initial_centroid = np.random.rand(n_clusters, X_train.shape[1])\n",
        "  n_neighbors=5\n",
        "\n",
        "  # Task 1: Data Preprocessing\n",
        "  X_train = z_score_normalization(X_train)\n",
        "  X_test = z_score_normalization(X_test)\n",
        "  print(\"The normalized features of the first 5 samples of X_train are: \", X_train[:5, :])\n",
        "\n",
        "  # Task 2.1: Manhattan distance\n",
        "  m_distance = manhattan_distance(X_train, X_test)\n",
        "  print('The manhattan distance between the first 5 X_train and first 5 X_test are', m_distance[:5, :5])\n",
        "\n",
        "  # Task 2.2: Cosine distance\n",
        "  c_distance = cosine_distance(X_train, X_test)\n",
        "  print('The cosine distance between the first 5 X_train and first 5 X_test are', c_distance[:5, :5])\n",
        "\n",
        "  # Task 3: Classification based on the Nearest Neighbors\n",
        "  prediction, y_classes = knn_prediction(m_distance, y_train, n_neighbors)\n",
        "  print('The predicted classes for the first 10 samples in X_test are ', prediction[:10])\n",
        "  print('The true classes of these samples are ', y_test[:10].flatten())\n",
        "\n",
        "  # Task 4: Evaluation of the K Nearest Neighbors Classifier\n",
        "  f_score_array = f_score(y_test.flatten(), prediction, y_classes)\n",
        "  print('The F-scores for each class are ', f_score_array)\n",
        "\n",
        "  # Task 5-6: K-Means Clustering\n",
        "  assignment, centroid = k_means_cluster_reassignment(X_train, initial_centroid, max_iterations=100)\n",
        "  print('With cluster reassignment as the stopping criteria, the assignments for the first 5 samples of X_train are: ', assignment[:5])\n",
        "  assignment, centroid = k_means_centroid_value(X_train, initial_centroid, max_iterations=100)\n",
        "  print('With centroid value as the stopping criteria, the assignments for the first 5 samples of X_train are: ', assignment[:5])\n",
        "\n",
        "  # Task 6.3: Evaluating the value of k\n",
        "  silhouette_avg = silhouette_score_for_k(X_train[:300, :], assignment[:300])\n",
        "  print('The silhouette coefficient is ', silhouette_avg)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
