{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9788mVl2GJPl"
      },
      "source": [
        "# Sketched Character Detection Using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd4oJBUVYSuL"
      },
      "source": [
        "COMP2211 Programming Assignment 2, Spring 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DHqKCKJYjNE"
      },
      "source": [
        "# Overview (no task to complete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yl5MfOOSRTp"
      },
      "source": [
        "## Problem Statement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjZTKCOO4YDg"
      },
      "source": [
        "Have you ever considered dancing with your sketched buddies? It is totally possible! In this programming assignment, we are going to take the very first step to extract the drawings from a paper, building our own CNN-based object detection model.\n",
        "\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/6675724/219223438-2c93f9cb-d4b5-45e9-a433-149ed76affa6.gif\" alt=\"teaser\" width=\"600\"/>\n",
        "\n",
        "\n",
        "Image credits: https://github.com/facebookresearch/AnimatedDrawings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx5CYVcK5lSp"
      },
      "source": [
        "Object detection is a standard task in computer vision. The goal is to identify regions of interest from an input image. In our case, we are going to extract the bounding box for the character (2nd subfigure from left to right). A bounding box is the smallest rectangle region that wraps the object of interest. With the bounding box, it will be easy to segment the character from the image, and then facilitate the downstream animation generation tasks.\n",
        "\n",
        "<img src=\"https://github.com/shellywhen/CNN-drawing-segmentation/blob/main/images/segmentation-illustration.gif?raw=true\" alt=\"segmentation\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS2YPG2a0_uE"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "citZm92Y4gvg"
      },
      "source": [
        "\n",
        "We are using the *Amateur Drawings Dataset*. The original dataset comprises over 178K (~50Gb) images and associated annotations of amateur drawings. Due to the limit of Google Colab, the assiduous TAs have prepared a smaller sample.\n",
        "\n",
        "<img src=\"https://github.com/shellywhen/CNN-drawing-segmentation/blob/main/images/dataset.png?raw=true\" alt=\"dataset_overview\" width=\"600\"/>\n",
        "\n",
        "The filtered dataset has 2K (~500Mb) amateur drawing images. And the JSON file records the annotation list. Each entry has the `src` field about the relative path to the image, and the `bbox` field is defined by `[x, y, width, height]`, where `x` and `y` is the horizontal and vertical position of the top left corner.\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"src\": \"images/1.png\",\n",
        "    \"bbox\": [\n",
        "        32.0,\n",
        "        56.888888888888886,\n",
        "        303.36507936507934,\n",
        "        428.69841269841265\n",
        "    ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyAfzNIN5wNp"
      },
      "source": [
        "Note that the coordination of a pixel-based image is different from the Cartesian coordinate system that you read in Mathemetics textbooks (see the figure below). The origin (0,0) is usually at the top-left corner of the image. The X-coordinate increases as you move to the right. The Y-coordinate increases as you move down.\n",
        "\n",
        "<img src=\"https://cdn-learn.adafruit.com/assets/assets/000/001/264/medium800/lcds___displays_coordsys.png?1396770439\" alt=\"image_coordination\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ-NH5uv1C7E"
      },
      "source": [
        "## Task Briefing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww7Ub3Mt61_w"
      },
      "source": [
        "To implement the CNN model, we will utilize the `Tensorflow` and `Keras` packages. They are well-suited for individuals new to machine learning. For detailed information, please consult the official [documentation](https://www.tensorflow.org/api_docs/python/tf) or refer to an [example](https://www.tensorflow.org/hub/tutorials/object_detection).\n",
        "\n",
        "We will also take advantage of a pre-trained model (a machine learning model that has been trained on a large dataset to perform specific tasks) -- the well-known ResNet. Using a pretrained model instead of training one from scratch is like having a seasoned chef prepare the base of your dish, rather than starting to cook without any prior kitchen experience. It has encapsulated rich knowledge from a general dataset of natural images in its parameters. We will use our unique dataset to combat its limited understanding of children's amateur drawings. To achieve this, we need to modify the model architecture to fit in our object detection task, while freezing its weights.\n",
        "\n",
        "In this programming assignment, we will follow three procedures.\n",
        "\n",
        "- Data loading\n",
        "  - Load the dataset\n",
        "  - Transform the data to fit in the model\n",
        "     - **Task 1**: <u>normalize the bounding box feature</u> (graded for accuracy)\n",
        "  - Split the data into training and testing dataset\n",
        "     - **Task 2**: <u>split the dataset</u> (graded for accuracy)\n",
        "- Model compilation & training\n",
        "  - Build the object detection model based on ResNet\n",
        "     - **Task 3**: <u>add the convolution layer</u> (graded for accuracy)\n",
        "     - **Task 4**: <u>add the output layer</u> (graded for accuracy)\n",
        "     - **Task 5**: <u>define the mean square loss function</u> (graded for accuracy)\n",
        "     - **Task 6**: <u>define the GIoU loss function</u> (graded for accuracy)\n",
        "  - Train the model\n",
        "     - **Task 7**: <u>export trained parameters for plagiarism check</u>\n",
        "- Performance evaluation\n",
        "  - Examine model performance\n",
        "     - **Task 8**: <u>compute loss and provide the poorest sample</u> (graded for accuracy)\n",
        "  - Admire our powerful object detector üòé\n",
        "     - **Task 9**: <u>use the model for new images</u> (graded for accuracy)\n",
        "     - **Task 10**: <u>test out the model</u> (graded for performance)\n",
        "\n",
        "After finishing all tasks, you should have four files for testing and grading: `proof.pkl`, `grading_prediction.pkl`, `self_testing_prediction.pkl`, and `pa2_task.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEknUkHg6vLB"
      },
      "source": [
        "## Tips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3IIyEHx6qvQ"
      },
      "source": [
        "Please be aware that the files you upload to Colab won‚Äôt be available forever. Colab is a temporary environment with an idle timeout of 90 minutes and an absolute timeout of 12 hours. This means that the runtime will disconnect if it has remained idle for 90 minutes, or if it has been in use for 12 hours. On disconnection, you lose all your variables, states, installed packages, and files and will be connected to an entirely new and clean environment on reconnecting. Also, Colab has a disk space limitation of 108 GB, of which only 77 GB is available to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv1I1K9IrxiM"
      },
      "source": [
        "# üî• Preliminaries\n",
        "\n",
        "Let's set up the GPU environment to accelerate model training and suppress unnecessary system logs. First of all, you'll need to enable GPUs for the notebook <a href=\"https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\" target=\"_blank\">(Learn how)</a>\n",
        "- Navigate to `Edit` ‚Üí `Notebook Settings`\n",
        "- select `GPU` from the `Hardware Accelerator` drop-down\n",
        "\n",
        "As there is limited free GPU quota in Colab, you may start training when you make sure everything is ready based on CPU.\n",
        "\n",
        "\n",
        "**What does it mean to train models based on GPU and why?**\n",
        "\n",
        "During training, the model iteratively adjusts its weights to minimize the difference between its predicted outputs and the true labels of the training examples. This process involves a lot of mathematical computations, especially matrix multiplications and convolutions. GPUs (Graphics Processing Units) are highly efficient at performing these types of computations, making them ideal for training CNN models. A GPU is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. While GPUs were originally designed for rendering graphics, they have evolved to become highly efficient at performing parallel computations, making them suitable for a wide range of applications beyond graphics processing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA9ap3H9eAMX",
        "outputId": "15a4c7f3-ee7a-41af-f8f3-b9a9a30eaad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Suppress warnings and logs\n",
        "import warnings\n",
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check whether a GPU environment is enabled\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if len(device_name) > 0 and __name__ == '__main__':\n",
        "    print(\"Found GPU at: {}\".format(device_name))\n",
        "elif __name__ == '__main__':\n",
        "    device_name = \"/device:CPU:0\"\n",
        "    print(\"No GPU, using {}.\".format(device_name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmJSolaNmFv7"
      },
      "source": [
        "# Data Loading (2 Tasks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAoKQZ7ksSKz"
      },
      "source": [
        "Here, we are going to download the dataset, transform the data format, and split it into the training and testing dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MflXH5nMRe-V"
      },
      "source": [
        "**Task 1** ‚Äî Normalize the bounding box feature\n",
        "\n",
        "You should normalize the bbox into range [0, 1] according to the proportion to (img_width, img_height). That is, normalize the values according to the formula:\n",
        "\n",
        "- x' = x / img_width\n",
        "- y' = y / img_height\n",
        "- width' = width / img_width\n",
        "- height' = height / img_height\n",
        "\n",
        "For example, suppose we have an image of size (100, 200), and a bbox with x = 30, y = 40, width = 50, height = 80, then we should normalize it to x = 30/100 = 0.3, y = 40/200 = 0.2, width = 50/100 = 0.5, height = 80/200 = 0.4.\n",
        "\n",
        "After normalizing the values, append the list [x, y, width, height] to the bboxes list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sdloOu0npDU"
      },
      "source": [
        "üî• **In your Python submission, you should comment out (or remove) statements with `!` as its start or simply remove the following block. Or there will be syntax errors.** There are three places in this `ipynb` that you should be careful about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Td51TWAvu6i"
      },
      "outputs": [],
      "source": [
        "# YOU HAVE TO COMMENT OUT THIS CELL WHEN HANDING IN THE CODE\n",
        "\n",
        "# Download the dataset from Amazon S3; It can take 1 min.\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/gt_bbox.json\"\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/images.zip\"\n",
        "\n",
        "!unzip -q images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZmKkff52eUm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def preprocess_dataset(dataset, image_size=(224, 224)):\n",
        "  \"\"\"Preprocess the raw dataset\n",
        "  dataset: the parsed JSON file as the dataset\n",
        "  image_size: targeted image size for the model input\n",
        "\n",
        "  return an array of the image paths, the transformed images and ground truth bboxes\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  bboxes = []\n",
        "  urls = []\n",
        "\n",
        "  for item in dataset:\n",
        "    url = item['src']\n",
        "    urls.append(url)\n",
        "    original_image = load_img(item['src'], target_size=None)\n",
        "    original_width = original_image.width\n",
        "    original_height = original_image.height\n",
        "    image = original_image.resize(image_size)\n",
        "    image = img_to_array(image)\n",
        "    image = preprocess_input(image)\n",
        "    images.append(image)\n",
        "\n",
        "    bbox = item['bbox']\n",
        "    x, y, width, height = bbox\n",
        "\n",
        "    # Task 1: You should normalize the bbox into range [0, 1] according to the proportion to (width, height)\n",
        "    ###### TO DO ######\n",
        "\n",
        "    ###################\n",
        "  urls = np.array(urls, dtype=str)\n",
        "  images = np.array(images, dtype=np.float32)\n",
        "  bboxes = np.array(bboxes, dtype=np.float32)\n",
        "  return urls, images, bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48BpNqgFq2_2"
      },
      "source": [
        "**Task 2** ‚Äî Split the dataset\n",
        "\n",
        "You should split the dataset with `n` instances into two parts. One for training and one for testing. The training data and testing data should not overlap. Your function should work on an arbitrary input dataset of `np.ndarray`.\n",
        "\n",
        "Based on the `test_split_ratio` ranged [0, 1], the target size of the testing dataset is `test_size = int(n * test_split_ratio)`. And the target size of the training dataset is `train_size = n - int(n * test_split_ratio)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQr_XQpZdesk"
      },
      "outputs": [],
      "source": [
        "def split_dataset(urls, x, gt, test_split_ratio=0.2):\n",
        "    \"\"\"Split the dataset according to the test split ratio\n",
        "    urls: the input image paths\n",
        "    x: the input image data (np.ndarray) to be fed into model\n",
        "    gt: the ground truth boundinng box (np.ndarray)\n",
        "    test_split_ratio: the percentage of test dataset size in respect to the full dataset\n",
        "\n",
        "    return the train_url, train_x, train_y, and test_url, test_x, and test_y\n",
        "    \"\"\"\n",
        "    n = x.shape[0]\n",
        "    test_size = int( n * test_split_ratio)\n",
        "\n",
        "    # Task 2: You should split the data into train data and test data according to test_size\n",
        "    ###### TO DO ######\n",
        "\n",
        "    ###################\n",
        "    return train_url, train_x, train_y, test_url, test_x, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNIKSM-ZOxiu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  N = 1000  # we only load N images for the assignment (max=2,000)\n",
        "  IMAGE_SIZE = (224, 224) # ResNet standard image size\n",
        "  DATASET_FILE = './gt_bbox.json'\n",
        "  SPLIT_TEST_RATIO = 0.2\n",
        "\n",
        "  with open(DATASET_FILE) as f:\n",
        "      dataset = json.load(f)[:N]\n",
        "  urls, images, bboxes = preprocess_dataset(dataset, image_size=IMAGE_SIZE)\n",
        "  train_url, train_x, train_y, test_url, test_x, test_y = split_dataset(urls, images, bboxes, SPLIT_TEST_RATIO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2DGwblQZHB8"
      },
      "source": [
        "# Model Compilation & Training (5 tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pVnm5bnkLET"
      },
      "source": [
        "When using ResNet50 for classification, the model expects a fixed-size input image and outputs a 1D array where each entry corresponds to the probability that the image belongs to a particular class. However, for our task, which is bounding box prediction, the standard ResNet50 architecture needs to be adapted. Our goal is not to classify an entire image into a single category, but rather to predict the coordinates of a box that encloses an object within the image. These coordinates are usually represented by four values: (x, y, width, height).\n",
        "\n",
        "Therefore, we will remove its original output layer and add custom layers for our task. Check the [Functional API](https://www.tensorflow.org/guide/keras/functional_api) in Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwTIpWnWu2pP"
      },
      "source": [
        "Please finish the function create_model() to set up the model architecture. It begins by loading the ResNet50 model with pre-trained weights from ImageNet.\n",
        "\n",
        "**Task 3** ‚Äî Add a convolutional layer to the model.\n",
        "\n",
        "Based on the output from previous layers, please set up a convolutional layer with 128 filters, kernel size = (3, 3), activated by 'relu', where the output vector should have the same size as the input.\n",
        "\n",
        "**Task 4** ‚Äî Add a fully connected layer to the model.\n",
        "\n",
        "Based on the output from previous layers, please add a fully connected layer to output the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2_wNY_9tNMO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Conv2D, Dropout\n",
        "\n",
        "def create_model(input_shape=(224, 224, 3)):\n",
        "  \"\"\"Create a CNN model for predicting the bounding box for drawings in an image\n",
        "  input_shape: the targeted image size\n",
        "\n",
        "  return the model architecture\n",
        "  \"\"\"\n",
        "  # load the pre-trained ResNet50 model without the top classification layer\n",
        "  base_model = ResNet50(weights='imagenet', input_shape=input_shape, include_top=False)\n",
        "  # freeze the base model layers\n",
        "  base_model.trainable = False\n",
        "  # add custom layers on top for bounding box prediction\n",
        "  model_x = base_model.output\n",
        "\n",
        "\n",
        "  # Task 3: You should add a convolutional layer with 128 filters, kernel size = (3, 3), activated by 'relu', output the same size\n",
        "  ###### TO DO ######\n",
        "\n",
        "  ###################\n",
        "\n",
        "  model_x = GlobalAveragePooling2D()(model_x)  # use global average pooling to flatten the output\n",
        "  model_x = Dropout(0.5)(model_x) # randomly drop out weights to avoid overfitting\n",
        "  model_x = Dense(64, activation='relu', kernel_initializer='random_normal', name=\"check_layer_1\")(model_x)  # add a fully connected layer\n",
        "  model_x = Dense(32, activation='relu', kernel_initializer='random_normal', name=\"check_layer_2\")(model_x)  # add a fully connected layer\n",
        "\n",
        "  # Task 4: add a fully connected layer for predicting (x, y, w, h) for bounding box\n",
        "  ###### TO DO ######\n",
        "\n",
        "  ###################\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=model_x)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tsTnV_kHIYv"
      },
      "source": [
        "**Task 5** ‚Äî Compute the sum of square error.\n",
        "\n",
        "We will use the square error as our loss function. For the predicted bbox and the ground truth bbox, the loss is calculated by the square distance between their top left (tl) corner position and the bottom right (br) corner.\n",
        "\n",
        "$$\\text{SE} = ||\\text{pred}_\\text{tl} - \\text{gt}_\\text{tl}||^2 + ||\\text{pred}_\\text{br} - \\text{gt}_\\text{br}||^2  $$\n",
        "\n",
        "To enable gradient descend in the training phase, the input variables are `tf.Tensor`. Therefore, to compute square values, please use `tf.square`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJzKD8TJHXk5"
      },
      "outputs": [],
      "source": [
        "def se_func(x1_pred, y1_pred, w_pred, h_pred, x1_gt, y1_gt, w_gt, h_gt):\n",
        "  \"\"\"Compute the square error (SE) based on the tensor of (x, y, w, h) for the predicted bbox and ground truth\n",
        "\n",
        "  return an (N, 1) tensor as the individual loss value\n",
        "  \"\"\"\n",
        "  # Task 5: Compute the SE loss\n",
        "  ###### TO DO ######\n",
        "\n",
        "  ###################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn5mKZB-aAiu"
      },
      "source": [
        "**Task 6** ‚Äî Compute the GIoU loss given the x, y, width, height of the predicted value and the ground truth.\n",
        "\n",
        "In addition to MSE, the [generalized intersection over union (GIoU)](https://giou.stanford.edu/) loss function is also highly useful when training a bounding box prediction model. Its major steps are as follows.\n",
        "1. Calculate the intersection ($\\text{I}$) and the union ($\\text{U}$) of the predicted and ground truth bounding boxes.\n",
        "2. Find the coordinates of the enclosing box ($\\text{C}$), i.e., the smallest box that would enclose both the predicted and ground truth bounding boxes.\n",
        "3. The GIoU is then calculated by subtracting the normalized area of the difference between the enclosing box and the union from the IoU.\n",
        "4. Get the GIoU loss, defined by 1 - GIoU.\n",
        "\n",
        "<img src=\"https://github.com/shellywhen/CNN-drawing-segmentation/blob/main/images/giou.png?raw=true\" alt=\"bbox relationship\" width=\"600\"/>\n",
        "\n",
        "To avoid divide-by-zero problem, you may add a small constant ($c=1\\mathrm{e}-7$) to the denominator.\n",
        "\n",
        "$$\\text{IoU} = \\frac{\\text{Area of I}}{\\text{Area of U} + c}$$\n",
        "\n",
        "$$\\text{GIoU} = \\text{IoU} - \\frac{\\text{Area of C - Area of U}}{\\text{Area of C + c}} $$\n",
        "\n",
        "$$\\text{GIoU Loss} = 1 - \\text{GIoU} $$\n",
        "\n",
        "\n",
        "To enable gradient descend in the training phase, the input variables are `tf.Tensor`. Therefore, to compute maximum or minimum, please use `tf.maximum` and `tf.minimum`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B1iXuDLXr11"
      },
      "outputs": [],
      "source": [
        "def giou_func(x1_pred, y1_pred, w_pred, h_pred, x1_gt, y1_gt, w_gt, h_gt):\n",
        "  \"\"\"Compute the giou loss based on the tensor of (x, y, w, h) for the predicted bbox and ground truth\n",
        "\n",
        "  return an (N, 1) tensor as the loss value\n",
        "  \"\"\"\n",
        "  # Task 6: Compute the GIOU loss given the prediction and the ground truth bbox data\n",
        "  ###### TO DO ######\n",
        "\n",
        "  ###################\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SJoXryfHDP1"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable(name=\"loss\")\n",
        "def loss_func(pred, gt):\n",
        "  \"\"\"The loss function for model training.\n",
        "  pred: an (N, 4) numpy array of predicted value for x, y, w, h\n",
        "  gt: an (N, 4) numpy array of the ground truth value x, y, w, h\n",
        "\n",
        "  return a scalar value of the mean batch loss\n",
        "  \"\"\"\n",
        "  gt = tf.convert_to_tensor(gt, dtype=tf.float32)\n",
        "  pred = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
        "\n",
        "  x1_gt, y1_gt, w_gt, h_gt = tf.split(gt, 4, axis=-1)\n",
        "  x1_pred, y1_pred, w_pred, h_pred = tf.split(pred, 4, axis=-1)\n",
        "\n",
        "  # you can also try using the mse error\n",
        "  # loss = se_func(x1_pred, y1_pred, w_pred, h_pred, x1_gt, y1_gt, w_gt, h_gt)\n",
        "  loss = giou_func(x1_pred, y1_pred, w_pred, h_pred, x1_gt, y1_gt, w_gt, h_gt)\n",
        "  return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR_7Ei1EW2v0"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  model = create_model()\n",
        "  model.compile(optimizer='adam', loss=loss_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCLU_TvGcj4j"
      },
      "source": [
        "You may run the following code to enable an advanced callback of TensorBoard. It is a visualization suite for models. You may inspect the real-time training logs through [scalar panel](https://www.tensorflow.org/tensorboard/scalars_and_keras) or gain a better understanding of the model architecture through the [model graph panel](https://www.tensorflow.org/tensorboard/graphs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaqR0Fem5NpN"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "if __name__ == '__main__':\n",
        "  # set up the tensorboard callback for reviewing real-time progress\n",
        "  log_dir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajtuJK2oKHsL"
      },
      "outputs": [],
      "source": [
        "# load the tensorboard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyI5lrXoMw9K"
      },
      "outputs": [],
      "source": [
        "# remember to click the refresh button to load updated logs while training.\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIljSChMIiWn"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  # train the model, you may reduce the epoch number to test the functionability at first\n",
        "  # as ResNet is a relatively large model, for 20 epochs, you may wait for 2 min using GPU, or 30 min using CPU\n",
        "  N_EPOCH = 20\n",
        "  model.fit(train_x, train_y, epochs=N_EPOCH, batch_size=64, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG0j5Vrww_VX"
      },
      "source": [
        "**Task 7**: Export a proportion of the model parameters.\n",
        "\n",
        "Run the following cell and download the model pameters to hand in for plagiarism checking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CByhPX_zj9Y9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def get_param_from_layer(model, layer_name):\n",
        "    layer = model.get_layer(name=layer_name)\n",
        "    weights = layer.get_weights()\n",
        "    # Concatenate the weights into a single numpy array\n",
        "    params = np.concatenate([w.flatten() for w in weights])\n",
        "    return params\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  param_1 = get_param_from_layer(model, 'check_layer_1').reshape((1, -1))\n",
        "  param_2 = get_param_from_layer(model, 'check_layer_2').reshape((1, -1))\n",
        "  proof = np.concatenate([param_1.T, param_2.T], axis=0)\n",
        "  with open('proof.pkl', 'wb') as f:\n",
        "    pickle.dump(proof, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA8ALhuapzSD"
      },
      "outputs": [],
      "source": [
        "# You can run this to save the model to your local machine to avoid re-training\n",
        "if __name__ == '__main__':\n",
        "  filename = 'model.keras'\n",
        "  model.save(filename)\n",
        "  model = tf.keras.models.load_model(filename, safe_mode=False, custom_objects={'loss_func': loss_func})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf5G4MnaZrEL"
      },
      "source": [
        "# Performance Evaluation (3 tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYEw_LUe_0NF"
      },
      "source": [
        "We will examine the model performance based on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH6XooDo_p2w"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  results = model.evaluate(test_x, test_y)\n",
        "  print(f'The loss in the model: {results}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4YzXtk2tMjd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "\n",
        "def visualize_bounding_box(image_path, predicted_bbox=None, groundtruth_bbox=None):\n",
        "  \"\"\"Plot the original image, the predicted bounding box, and groundtruth (if any)\n",
        "  image_path: the path to the image\n",
        "  predicted_bbox: the predicted bounding box for drawings in the image\n",
        "  groundtruth_bbox: the ground truth bounding box\n",
        "\n",
        "  return void\n",
        "  \"\"\"\n",
        "  image = cv2.imread(image_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  fig, ax = plt.subplots(1, figsize=(5, 5))\n",
        "  ax.imshow(image)\n",
        "\n",
        "  def draw_boxes(bbox, color, label):\n",
        "    x, y, w, h = bbox\n",
        "    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=color, facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(x, y-18, f'{label}', fontsize=12, verticalalignment='top', color=color)\n",
        "\n",
        "  if predicted_bbox is not None:\n",
        "      draw_boxes(predicted_bbox, 'red', 'Predicted')\n",
        "  if groundtruth_bbox is not None:\n",
        "      draw_boxes(groundtruth_bbox, 'green', 'Ground Truth')\n",
        "\n",
        "  plt.axis('off')\n",
        "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP1cnOJ36TQF"
      },
      "source": [
        "**Task 8** ‚Äî Extract failure cases.\n",
        "\n",
        "We are going to load a list of test data and associated ground truth.Please find out the one with the worst loss value.\n",
        "\n",
        "For grading, please do not print anything else. It is guaranteed that the worst loss value only occurs once.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDn1jIuAkuNl"
      },
      "outputs": [],
      "source": [
        "def extract_failure_case(loss_function, pred, gt, k=5):\n",
        "  \"\"\" Extract failure cases with the worst performance.\n",
        "  model: the trained model\n",
        "  loss_function: a loss function that provides (n,) loss value for (n, 4) prediction and (n, 4) results\n",
        "  pred: the predicted bounding box (scaled to the image size)\n",
        "  gt: the ground truth bbox in the test dataset\n",
        "  k: the number of top failures with the largest loss value\n",
        "\n",
        "  return the indexes to the top k failure case\n",
        "  \"\"\"\n",
        "  # Task 8: Extract failure cases\n",
        "  ###### TO DO ######\n",
        "\n",
        "  ###################\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rescale_bbox(src, raw_pred, scaled_gt):\n",
        "  original_image = load_img(src, target_size=None)\n",
        "  original_width = original_image.width\n",
        "  original_height = original_image.height\n",
        "  def _rescale_bbox(bbox):\n",
        "    x, y, w, h = bbox\n",
        "    x = x * original_width\n",
        "    y = y * original_height\n",
        "    w = w * original_width\n",
        "    h = h * original_height\n",
        "    return np.array([x, y, w, h])\n",
        "  return _rescale_bbox(raw_pred), _rescale_bbox(scaled_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd9TmRAjLo3N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  test_pred = model.predict(test_x)\n",
        "  n_failures = 20\n",
        "  worst_case_indexes = extract_failure_case(mean_absolute_error, test_pred, test_y, n_failures)\n",
        "  for k in range(n_failures):\n",
        "    idx = worst_case_indexes[k]\n",
        "    test_pred_instance, gt_instance = rescale_bbox(test_url[idx], test_pred[idx], test_y[idx])\n",
        "    visualize_bounding_box(test_url[idx], test_pred_instance, gt_instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7CuVbmt8h35"
      },
      "source": [
        "**Task 9** ‚Äî Use the model based on an image input.\n",
        "You are provided with the resized raw image data and a model. Please preprocess the image and use the model to predict the bounding box of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-6GVY1wMGom"
      },
      "outputs": [],
      "source": [
        "def predict_bounding_box(model, src):\n",
        "  \"\"\"leverage the model to identify drawings in an image\n",
        "  model: a trained object detector model\n",
        "  src: the source of the image file\n",
        "\n",
        "  return [x, y, w , h] for the predicted bounding box in the original image\n",
        "  \"\"\"\n",
        "  original_image = load_img(src, target_size=None)\n",
        "  original_width = original_image.width\n",
        "  original_height = original_image.height\n",
        "  image = original_image.resize((224, 224))\n",
        "  image = img_to_array(image)\n",
        "\n",
        "\n",
        "  # Task 9: Using the input model to predict the bounding box [x, y, width, height] of a drawing in the input image data\n",
        "  ###### TO DO ######\n",
        "\n",
        "  ###################\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVwMUveqcBw_"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "def inspect(model, dataset, k=None):\n",
        "  \"\"\"Visually inspect one instance with its predicted and ground truth bbox\n",
        "  model: the trained model\n",
        "  dataset: the full dataset with image source and ground truth\n",
        "  k: the index of the image under inspection\n",
        "\n",
        "  return void\n",
        "  \"\"\"\n",
        "  if k is None:\n",
        "    k = randint(0, len(dataset)-1)\n",
        "  src = dataset[k]['src']\n",
        "  ground_truth = dataset[k]['bbox']\n",
        "  pred = predict_bounding_box(model, src)\n",
        "  visualize_bounding_box(src, pred, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24qDYydVSeNm"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  inspect(model, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPx4aSU4nUjt"
      },
      "outputs": [],
      "source": [
        "# YOU MUST COMMENT OUT THIS CELL WHEN HANDING IN THE CODE\n",
        "\n",
        "# Let us take a look at how your model performs on the drawings of Desmond's kids XD!\n",
        "!wget -O earnest.jpg \"https://home.cse.ust.hk/~desmond/images/ernest/peacock.jpg\" > /dev/null 2>&1\n",
        "!wget -O elvis.jpg \"https://home.cse.ust.hk/~desmond/images/elvis/elvis-pokemon.jpg\" > /dev/null 2>&1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  earnest_pred = predict_bounding_box(model, \"earnest.jpg\")\n",
        "  visualize_bounding_box(\"earnest.jpg\", earnest_pred, None)\n",
        "  elvis_pred = predict_bounding_box(model, \"elvis.jpg\")\n",
        "  visualize_bounding_box(\"elvis.jpg\", elvis_pred, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7X_23HxtA3"
      },
      "source": [
        "**Task 10** ‚Äî Predict the bounding box on given data for grading\n",
        "\n",
        "Run the following cell and download the generated data to hand in.\n",
        "You can download the files from the [side panel](https://stackoverflow.com/questions/48774285/how-to-download-file-created-in-colaboratory-workspace) and submit it using the export `.py` file. You may need to refresh the panel before seeing the file.\n",
        "\n",
        "<img src=\"https://github.com/shellywhen/CNN-drawing-segmentation/blob/main/images/instruction.png?raw=true\" alt=\"dataset_overview\" width=\"500\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdeSEahcmtmo"
      },
      "outputs": [],
      "source": [
        "# YOU MUST COMMENT OUT THIS CELL WHEN HANDING IN THE CODE\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/grading_paths.pkl\"\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/grading_images.zip\"\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/testing_paths.pkl\"\n",
        "!wget \"https://comp2211-pa2.s3.ap-northeast-2.amazonaws.com/testing_images.zip\"\n",
        "\n",
        "!unzip -q grading_images.zip\n",
        "!unzip -q testing_images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaD2VWTaj-0e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Task 10: Download 'grading_pred.pkl' and `testing_pred.pkl`\n",
        "if __name__ == '__main__':\n",
        "  with open('grading_paths.pkl', 'rb') as f:\n",
        "    paths = pickle.load(f)\n",
        "  grading_pred = np.array([predict_bounding_box(model, path) for path in paths])\n",
        "  with open('grading_pred.pkl', 'wb') as f:\n",
        "    pickle.dump(grading_pred, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  with open('testing_paths.pkl', 'rb') as f:\n",
        "    paths = pickle.load(f)\n",
        "  testing_pred = np.array([predict_bounding_box(model, path) for path in paths])\n",
        "  with open('testing_pred.pkl', 'wb') as f:\n",
        "    pickle.dump(testing_pred, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QqaLEMIvKWQ"
      },
      "outputs": [],
      "source": [
        "# Note: For grading on Zinc, please zip `grading_pred.pkl`, `proof.pkl`, and `pa2_task.py`.\n",
        "# Note: For testing on Zinc, please zip `testing_pred.pkl`, `proof.pkl`, and `pa2_task.py`.\n",
        "# When exporting this `.ipynb` to a `.py` file, you should comment out cells with statements starting with \"!\" or \"%\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajodRJxIGtCN"
      },
      "source": [
        "# Learn More (no task to complete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsgBC84vY-mz"
      },
      "source": [
        "So, what's next with the extracted drawings? You may dive into this SIGGRAPH [paper](https://arxiv.org/pdf/2303.12741.pdf) for more details. ü§´ It is not as difficult as you may expect. In general, there are three more steps, including segmentation, joint keypoint estimation, rig generation, and motion retargeting.\n",
        "\n",
        "<img src=\"https://github.com/shellywhen/CNN-drawing-segmentation/blob/main/images/pipeline.png?raw=true\" alt=\"pipeline\" width=\"960\"/>\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/shellywhen/CNN-drawing-segmentation/blob/main/images/retarget.png?raw=true\" alt=\"retarget\" width=\"960\"/>\n",
        "\n",
        "You can also play with the [online demo](https://sketch.metademolab.com/) or install the [application](https://github.com/facebookresearch/AnimatedDrawings/tree/main) to create your own dancing buddy. Do you know what is even cooler? Try to create more magic with your own modelsüèÇ!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
