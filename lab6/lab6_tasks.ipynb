{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMP 2211 Exploring Artificial Intelligence** #\n",
    "## Lab 6: Multi-Layer Perceptron (MLP) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mlp.jpg](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjH2GbE1DXODQ8tnCVkn8S7F4KiCfaC0RvFt3I2L8d6YQplfHbKWGkaU20EwY7zWj9TjLxn1WdKey8kpqnox_KAKmS3qn0fN2kRLht9tr1bGNkcLgNiK3IUKJiAYd1bP5IXsaKc5s__4H0nLkLw2m2kNLoOU7R_hMpBXfDES-NxIFPPY_RBOpUkoUt-aQ/w613-h345/Multi-layer%20perceptron%20intro%20image.png \"Image Source: https://www.pycodemates.com/2023/01/multi-layer-perceptron-a-complete-overview.html\")\n",
    "\n",
    "<font size=\"1\">Image Source: https://www.pycodemates.com/2023/01/multi-layer-perceptron-a-complete-overview.html</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Mount Google Drive and Import Libraries\n",
    "Download and save a copy of the Lab6 Notebook and Datasets (*test_labels.csv*) to your Google Drive, ensuring that all these files are in the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  from google.colab import drive\n",
    "  drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the pathway of the *%cd* shell command according to the location in your Google Drive where you have saved the Notebook and datasets. For example, if you saved the files the files in the location `MyDrive/lab6`: \n",
    "\n",
    "\n",
    "then you can run the following `%cd` code to access that directory:\n",
    "\n",
    "```\n",
    "%cd \"/content/drive/MyDrive/lab6\"\n",
    "```\n",
    "\n",
    "You can also run the following code to ensure you are in the correct working directory.\n",
    "\n",
    "```\n",
    "%ls\n",
    "```\n",
    "\n",
    "It should display the filenames of *HKProp_Dataset.csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please delete the \"# ADD YOUR PATH HERE\" after you update the path\n",
    "%cd \"/content/drive/MyDrive/\"+ # ADD YOUR PATH HERE\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab activity, we will use the **Numpy** and **Pandas** library to load and process data, and use **folium** and **plotly** for visualization. Please import these by running the code below.\n",
    "\n",
    "**Pandas** is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. If you want to know more about **Pandas**, you can check the documentation in https://pandas.pydata.org/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "As a partner of a property investment company, your objective is to make a profit from investing in and the eventual sale of invested properties. To do this, you need a solid property prediction model based on historical property transactions. To enable the prediction of future property prices from your prediction model compared against prevailing asking prices. So that the future sale of a property will bring in a nice profit.\n",
    "\n",
    "The Hong Kong Island areas of Central, Sheung Wan and Sai Wan private housing dataset is a Excel Comma Separated Value (CSV) text file, which include 29 features and 1139 observations. Each observation represents the sale of a home and each feature is an attribute describing the house or the circumstance of the sale. \n",
    "\n",
    "The features include string, boolean, categorical and numerical variables. These features are as follows:\n",
    "\n",
    "| Field Label | Field description | Data format | Remark |\n",
    "| --- | --- | --- | --- |\n",
    "| Reg_Date | Register date in Land Registry | DD/MM/YYYY |  |\n",
    "| Reg_Year | Register year | YYYY | Extract from Reg_Date field |\n",
    "| Prop_Name_ENG | Name of the property | String |  |\n",
    "| Address_ENG | Address of the property | String |  |\n",
    "| Prop_Type | Type of the property | Boolean (Single/ Estate) |  |\n",
    "| Tower | Tower number | String |  |\n",
    "| Floor | Floor of the property | Numeric |  |\n",
    "| Floor_(H/M/L) | Floor category (H/M/L) | Category (H/M/L) | Floor 0 to 15 => L, Floor 16 to 34 => M, Floor upper than 35 => H |\n",
    "| Flat | Flat of the property | String |  |\n",
    "| Bed_Room | No. of bed room of the property | Numeric |  |\n",
    "| Roof | Roof included? | Boolean (Y/N) |  |\n",
    "| Build_Ages | Building age at registration | String |  |\n",
    "| Rehab_Year | Year of building rehabilitation schemes | Numeric | [Website](https://www.ura.org.hk/en/project/rehabilitation/building-rehabilitation-schemes) |\n",
    "| Eff_Build_Age | Effective building age that consider the building rehabilitation schemes | String | If URA is 0, Eff_Age = Build_Ages. If URA is not 0, (Reg_Year)+{[Build_Age-(Reg_Year-URA)]*2/3} |\n",
    "| SalePrice_10k | Sale Price | Numeric |  |\n",
    "| SaleableArea | Saleable area of the property | Numeric |  |\n",
    "| Gross Area | Gross area of the property | Numeric |  |\n",
    "| SaleableAreaPrice | Price per saleable area | Numeric |  |\n",
    "| Gross Area_Price | Price per gross area | Numeric |  |\n",
    "| Kindergarten | No. of kindergarten near the property | Numeric |  |\n",
    "| Primary_Schools | No. of primary schools near the property | Numeric |  |\n",
    "| Secondary_Schools | No. of secondary schools near the property | Numeric |  |\n",
    "| Parks | No. of public parks near the property | Numeric |  |\n",
    "| Library | No. of library near the property | Numeric |  |\n",
    "| Bus_Route | No. of bus stations near the property | Numeric |  |\n",
    "| Mall | No. of shopping mall near the property | Numeric | Shun Tak Centre, Western Market, Infinitus Plaza |\n",
    "| Wet Market | No. of wet market near the property | Numeric |  |\n",
    "| Latitude | The latitude of the property | Numeric | [Website](http://www.mapdevelopers.com/batch_geocode_tool.php) |\n",
    "| Longitude | The longitude of the property | Numeric | [Website](http://www.mapdevelopers.com/batch_geocode_tool.php) |\n",
    "| Edu_Inst | The education institution near the property | Numeric | Sum of Kindergarten, Primary_Schools and Secondary_Schools |\n",
    "\n",
    "\n",
    "\n",
    "Relevant information on the datasets can be retrieved by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Hong Kong Properties dataset \n",
    "pd.set_option('display.max_columns', None)\n",
    "dataframe = pd.read_csv('HKProp_Dataset.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the dataframe we used in this lab. The correct shape should be `(1100, 28)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape for the dataframe\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the Sale Price distribution\n",
    "\n",
    "Before we start our research, we first have a visualization to better understand the distribution of our data. The target value for this analysis is the sale price, which is `'SalePrice_10k'` in the dataframe. We will use the sale price of different properties as the radius for the circle marker and visualize them in the map. We use different color to plot the sale price for `(0,600)`, `(600,1000]`, and `(1000,+∞)`\n",
    "\n",
    "You can explore the sale price directly on the map. By clicking the circle on the map, you can see the name, type, and floor of different properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sale price\n",
    "hongkong_map = folium.Map(location=[22.281442,114.152991],\n",
    "                        zoom_start=15,\n",
    "                   tiles=\"cartodbpositron\")\n",
    "\n",
    "for i in range(len(dataframe)):\n",
    "    lat = dataframe['Latitude'].iloc[i] \n",
    "    long = dataframe['Longitude'].iloc[i] \n",
    "    SalePrice_10k = dataframe['SalePrice_10k'].iloc[i]\n",
    "    radius = min(SalePrice_10k/100, 20)\n",
    "\n",
    "    if SalePrice_10k > 1000:\n",
    "        color = \"#008080\"  # blue high price\n",
    "    elif SalePrice_10k < 600:\n",
    "        color = \"#9BCD9B\"  # grey cheap price\n",
    "    else:\n",
    "        color = \"#9C9C9C\"  # green normal price\n",
    "    \n",
    "    popup_text = \"\"\"Name : {}<br>\n",
    "                Type : {}<br>\n",
    "                Floor : {}<br>\"\"\"\n",
    "    \n",
    "    popup_text = popup_text.format(\n",
    "        dataframe['Prop_Name_ENG'].iloc[i], \n",
    "        dataframe['Prop_Type'].iloc[i], \n",
    "        dataframe['Floor'].iloc[i]\n",
    "        )\n",
    "\n",
    "    folium.CircleMarker(location = [lat, long], popup= popup_text,radius = radius, color = color, fill = True).add_to(hongkong_map)\n",
    "\n",
    "hongkong_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another direct visualization tool to check the locations for the most sold properties is to use a heatmap. You can zoom in or zoom out to find the location distribution of these sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(dataframe)\n",
    "lat = np.array(dataframe[\"Latitude\"][0:num])                       \n",
    "lon = np.array(dataframe[\"Longitude\"][0:num])                        \n",
    "rent_room = np.array(dataframe[\"SalePrice_10k\"][0:num],dtype=float)    \n",
    "\n",
    "data1 = [[lat[i],lon[i],rent_room[i]] for i in range(num)]    \n",
    "\n",
    "hongkong_map = folium.Map(location=[22.285442,114.152991],\n",
    "                        zoom_start=16) \n",
    "HeatMap(data1).add_to(hongkong_map) \n",
    "folium.TileLayer('cartodbpositron').add_to(hongkong_map)\n",
    "\n",
    "hongkong_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having a glance at the dataset distribution, you may now have a brief understanding of how the sale prices are connected to the locations. Here we preprocess and combine the features in the datasheet to construct training and validation data `X_trainval, y_trainval`, and testing data `X_test, y_test` for training and evaluating our MLP model. **Please DO NOT modify the code**. The correct shape should be `(1000, 166) (100, 166) (1000,) (100,)` for `X_trainval, X_test, y_trainval, y_test`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for generating input data and ground truth labels\n",
    "# Please DO NOT modify the code\n",
    "\n",
    "y = np.array(dataframe['SalePrice_10k'])\n",
    "\n",
    "# replace '--' with 0, replace $ with space.\n",
    "dataframe = dataframe.replace('--', '0').replace('$', '')  \n",
    "Reg_Date = pd.to_datetime(np.array(dataframe['Reg_Date']),infer_datetime_format=True)\n",
    "Days_to_reg_date = [int(t.days) for t in (Reg_Date - datetime.datetime.today())]\n",
    "Bedroom = dataframe['Bed_Room'].replace('Studio', '0').fillna(0)\n",
    "Is_studio = [1 if e == 0 else 0 for e in np.array(dataframe['Bed_Room'])] \n",
    "\n",
    "SaleableArea = [int(str(t).replace(',', '').replace('nan', '0')) for t in dataframe['SaleableArea']]\n",
    "SaleableAreaPrice = [int(str(t).replace(',', '').replace('$', '0').replace('nan', '0')) for t in dataframe['SaleableAreaPrice']]\n",
    "GrossArea = [int(str(t).replace(',', '').replace('nan', '0')) for t in dataframe['Gross Area']]\n",
    "GrossAreaPrice = [int(str(t).replace(',', '').replace('$', '0').replace('nan', '0')) for t in dataframe['Gross Area_Price']]\n",
    "\n",
    "X = np.concatenate([np.array(Days_to_reg_date).reshape(-1, 1), np.array(Bedroom).reshape(-1, 1),\n",
    "                        np.array(Is_studio).reshape(-1, 1), np.array(SaleableArea).reshape(-1, 1),\n",
    "                        np.array(GrossAreaPrice).reshape(-1, 1),\n",
    "                        np.array(GrossArea).reshape(-1, 1),\n",
    "                        np.array(SaleableAreaPrice).reshape(-1, 1),\n",
    "                        np.array(pd.get_dummies(dataframe[['Flat', 'Prop_Type', 'Tower', 'Roof']])),\n",
    "                       np.array(dataframe[['Floor', 'Build_Ages', 'Rehab_Year', 'Kindergarten', 'Primary_Schools', 'Secondary_Schools',\n",
    " 'Parks', 'Library', 'Bus_Route', 'Mall', 'Wet Market', 'Latitude', 'Longitude']].fillna(0))], axis=1)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = X[:1000], X[1000:], y[:1000], y[1000:]\n",
    "print(X_trainval.shape, X_test.shape, y_trainval.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Dataset Splitting and Preprocessing\n",
    "\n",
    "We will randomly shuffle and then split the training-validation dataset `(X_trainval, y_val)` into training set `(X_train, y_train)` and validation set `(X_val, y_val)` using the following two functions.\n",
    "\n",
    "**Task 1.1**: Implement a function `shuffle_data_numpy()`: The function takes the data `(X, y)` and `numpy_seed` as the input, and output the shuffled `(X_shuffle, y_shuffle)`. If the seed are fixed, the shuffled results are consistent across multiple runs. Note that you **CANNOT** use `sklearn.utils.shuffle`\n",
    "\n",
    "**Task 1.2**: Implement a function `train_val_split()`: The function should randomly shuffle the data with `shuffle_data_numpy` and then return split the data into training set `(X_train, y_train)` and validation set `(X_val, y_val)`. The length for `(X_train, y_train)` should be the given `train_size`. You will use this function to split the data `(X_trainval, y_trainval)`. Note that you **CANNOT** use `train_test_split` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT IMPORT ANY ADDITIONAL LIBRARY! (e.g. sklearn)\n",
    "def shuffle_data_numpy(X, y, numpy_seed):\n",
    "    # fix the random seed\n",
    "    np.random.seed(numpy_seed)\n",
    "\n",
    "    # TODO Task 1.1\n",
    "    # shuffle the given data pair (X, y)\n",
    "    # please use numpy functions so that the results are controled by np.random.seed(numpy_seed)\n",
    "\n",
    "    return X_shuffle, y_shuffle\n",
    "\n",
    "def train_val_split(X_trainval, y_trainval, train_size, numpy_seed):\n",
    "    # TODO TASK 1.2 \n",
    "    # apply shuffle on the data with given random seed, then split the data into training and validation sets\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `numpy_seed=42` and split the data into a training set with 700 samples and remaining as the validation set.\n",
    "\n",
    "If your function are implemented correctly, the `print()` should return `(700, 166) (300, 166) (700,) (300,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_val_split(X_trainval, y_trainval, 700, 42)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: MLP Model Building, Compiling, and Training\n",
    "\n",
    "**Task 2.1**: Implement the function `MyModel(num_dense_layer, dense_layer_unit, input_dim, dropout_ratio)` that builds an MLP model with the following input arguments as the setting:\n",
    "- `num_dense_layer`: the numbers of Dense layer in the model **excluding** the output layer(node).\n",
    "- `dense_layer_unit`: the number of units in the dense layer. Are all hidden layers using the same number of neurons.\n",
    "- `input_dim`: the input feature dimension.\n",
    "- `dropout_ratio`: the fraction of the input units to drop.\n",
    "\n",
    "Please also follows the following rules:\n",
    "- There are one `Dropout()` layer after one hidden `Dense` layer in the model. \n",
    "- The activation for hidden `Dense()` layer should be `ReLU`.\n",
    "- All layers are initialized using `uniform` kernel. (Not the `glorot_uniform`)\n",
    "- The activation for output layer(node) should be `linear`.\n",
    "- There should be `2*num_dense_layer+1` layers in total\n",
    "\n",
    "The function should return the built model. Please note that you do not need to call `model.compile()` or `model.fit()` in this function because they are the follow-up tasks.\n",
    "No additional import are allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No additional import allowed\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def MyModel(num_dense_layer, dense_layer_unit, input_dim, dropout_ratio):\n",
    "    # Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO Task 2.1\n",
    "    # Build your own model with model.add(), Dense layers, and Dropout layers\n",
    "    # Hint: you may consider using\n",
    "        # Dense(): https://keras.io/api/layers/core_layers/dense/\n",
    "        # Dropout(): https://keras.io/api/layers/regularization_layers/dropout/\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a MLP model with 2 `Dense` hidden layers. Each layer consists 40 units. Dropout are set to be `0` (No dropout)\n",
    "\n",
    "You can modify these setting to see how the architectures of MLP influence the performance.\n",
    "\n",
    "But Keep them as the default setting for the model for ZINC Submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep them as the default setting for the model you submitted to ZINC!\n",
    "num_dense_layer = 2\n",
    "dense_layer_unit = 40\n",
    "input_dim = len(X_train[0])\n",
    "dropout_ratio = 0\n",
    "\n",
    "model = MyModel(num_dense_layer, dense_layer_unit, input_dim, dropout_ratio)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**: Implement the function `MyModel_Training(model, X_train, y_train, X_val, y_val)` that compile and train the defined model\n",
    "- You should use `Adam` Optimizer with learning rate `1e-3`\n",
    "- The loss function for training is Mean Squared Error (`mse`), the metrics for evaluation is Mean Absolute Error (`mae`)\n",
    "- Return the **training history** and trained model. `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "\n",
    "Then you can start you training with different `batchsize` and `train_epoch` settings! \n",
    "\n",
    "The training arguments `batchsize` and `train_epoch` are the only two parameters you can change. \n",
    "\n",
    "We suggest not to use `train_epoch > 100` which will spend too much time for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def MyModel_Training(model, X_train, y_train, X_val, y_val, batchsize, train_epoch):\n",
    "\n",
    "    # TODO Task 2.2\n",
    "    # Compile and train the given model\n",
    "    # Hint: history can be returned by model.fit() function, please see https://keras.io/api/models/model_training_apis/\n",
    "\n",
    "    return history, model\n",
    "\n",
    "model = MyModel(num_dense_layer, dense_layer_unit, input_dim, dropout_ratio)\n",
    "\n",
    "batchsize = 8\n",
    "train_epoch = 50\n",
    "\n",
    "history, model = MyModel_Training(model, X_train, y_train, X_val, y_val, batchsize, train_epoch)\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test Mean Average Error (MAE): {test_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With proper choice for training epochs and batch size, the expected test Mean Average Error (MAE) should be lower than 80. Ideally, it should be around 70. (We only require MAE < 80 for grading)\n",
    "\n",
    "If the model you trained achieve the goal, please save the model by running following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./mlp_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the model you just saved and check the mean average error again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "mlp_model = load_model(\"./mlp_model.keras\")\n",
    "test_loss, test_mae = mlp_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test Mean Average Error (MAE): {test_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check of your MLP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['mae'], label='Training mae')\n",
    "plt.plot(history.history['val_mae'], label='Validation mae')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How accurate can your model predicted the sales price? Let's check!\n",
    "Here we plot the predicted sale price (y-axis) vs. ground truth sales price (x-axis). If you find most of the points distributed near the line `y=x`, your MLP model can help make a prediction for the house price!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test).flatten()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, test_predictions)\n",
    "plt.xlabel('Ground True Values for Sale Price (10k HKD)')\n",
    "plt.ylabel('Predictions for Sale Price (10k HKD)')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 3000], [-100, 3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission for Lab 6\n",
    "\n",
    "Please copy all the function you implemented into the file `lab6_tasks.py`, and submit it together with your `mlp_model.keras` model weight file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
